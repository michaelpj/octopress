<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Terms and Truth Conditions]]></title>
  <link href="http://termsandtruthconditions.herokuapp.com//atom.xml" rel="self"/>
  <link href="http://termsandtruthconditions.herokuapp.com//"/>
  <updated>2012-11-01T19:48:34+11:00</updated>
  <id>http://termsandtruthconditions.herokuapp.com//</id>
  <author>
    <name><![CDATA[Michael Peyton Jones]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Estimating the Effectiveness of DCP - 1]]></title>
    <link href="http://termsandtruthconditions.herokuapp.com//blog/2012/11/01/estimating-the-effectiveness-of-dcp-1/"/>
    <updated>2012-11-01T19:27:00+11:00</updated>
    <id>http://termsandtruthconditions.herokuapp.com//blog/2012/11/01/estimating-the-effectiveness-of-dcp-1</id>
    <content type="html"><![CDATA[<p><em>[This is the first of two posts I wrote on estimating the cost-effectiveness of the DCP organization, who themselves publish excellent reports on the cost-effectiveness of health interventions. It was originally published on the <a href="http://www.givingwhatwecan.org/blog/2012-10-27/estimating-the-effectiveness-of-dcp">Giving What We Can blog</a>.]</em></p>

<!-- more -->

<p>When searching for high-effectiveness charities, there are a few resources that incredibly useful. One of those is the reports of <a href="http://www.dcp2.org/main/Home.html">the Disease Control Priorities Project</a>, which provide an analysis of the cost-effectiveness of a great number of health interventions.</p>

<p>Given the value of this data, we might wonder just how valuable this information is, and whether it might be worth donating to DCP to produce more of it.</p>

<p>In this post I will present a statistical model (designed by myself and Nick Dunkley) for estimating the effectiveness of donating to DCP. Although this is going to over-simplify the situation, it should be able to give us a ballpark estimate.</p>

<p>Here&rsquo;s the idea. Firstly, we assume that DCP has some number of followers who make their donation decisions based on DCP&rsquo;s recommendations; that is, they put their donations towards whatever intervention DCP recommends most highly. If DCP says the best thing is buying hang-gliders for kittens, then that&rsquo;s where they&rsquo;ll send their money. So the current situation would have all that money going towards the intervention that the DCP2 report claims is most effective. We&rsquo;ll refer to the money donated as DCP&rsquo;s &ldquo;money moved&rdquo;. Furthermore, we&rsquo;ll assume that this money is all translated into DALYs at the rate that DCP claims.</p>

<p>Now, suppose that there&rsquo;s a big bag of interventions that DCP assesses. Then we assume that for some amount of money, DCP can pick a random intervention from the bag and accurately assess its effectiveness. If it comes out better their current best recommendation, then when they report this fact, all of their followers will shift their donations to the new best intervention.</p>

<p>If this is a good model of how DCP works, then we now have enough information to work out how much good we can do by donating to DCP. I&rsquo;m going to be using <a href="http://www.r-project.org/">R</a>, a statistics package, to do the heavy lifting -  it&rsquo;s free and I&rsquo;ve attached the script I used if you&rsquo;re keen and want to check my working!</p>

<p>Firstly, we need to know something about the distribution of the interventions in the bag. A quick reminder about probability distributions: the probability distribution of some random variable usually looks like a humped graph. Along the bottom are all the options we could have. In this case, that&rsquo;s the possible cost-effectivenesses of a random health intervention, i.e. pretty much any positive real number. Along the side it shows how likely that possibility is. One very common type of distribution is a &ldquo;normal&rdquo; distribution. This just looks like a big hump in the middle that spreads out to the sides (often called a &ldquo;bell-curve&rdquo;). Intuitively, that means that most of the time you get one of the possibilities in the middle, and the other possibilities get less likely as they get further away from the middle. Normal distributions are quite nice to work with, and they&rsquo;re also pretty common. Importantly, they can be totally specified by just two parameters: the mean of the distribution (the average outcome you get), and the standard deviation (which tells you how far away from the mean things tend to be). Between the two of these you know where the middle of the hump is (the mean), and how wide it is (the standard deviation), and that&rsquo;s all you need to know.</p>

<p>Back to our health interventions. Fortunately, we have a lot of data from DCP about the cost-effectiveness of interventions: if we plot the effectiveness of the interventions they assessed in the DCP2 report, we get a distribution that looks a bit like this: <img class="center" src="http://termsandtruthconditions.herokuapp.com//images/gwwc-dcp/interventions-distribution.png" /> That&rsquo;s definitely not a normal distribution, but it <em>is</em> a &ldquo;log-normal&rdquo; distribution. A log-normal distribution is fairly straightforward: if you take the logarithms of all the possibilities along the bottom, and then plot the distribution, you should get a normal distribution. And if we do that for the DCP data, then we do indeed get a very plausible-looking bell-curve. <img class="center" src="http://termsandtruthconditions.herokuapp.com//images/gwwc-dcp/interventions-distribution-log.png" /> Now, this makes it looks pretty plausible that the DCP data really does follow a (log-)normal distribution, and it&rsquo;ll be helpful if we can work out the two parameters for it that I mentioned above. For the moment, let&rsquo;s assume that taking the mean and standard deviation of the logarithms of the data gives us the right parameters - we can be more sophisticated (in particular we&rsquo;re going to want to think about the possibility of error), but I&rsquo;ll leave that for a later post.</p>

<p>Having the parameters for the distribution means that we can get R to make up entirely new data points <em>as if</em> they came from the original data set. And if the actual data really is distributed in the way we think it is, then this is a lot like DCP finding and assessing a new random intervention. We can then do the rest of our model process - check whether it&rsquo;s a better charity than we had before, and if so work out what the difference is - and get an answer for how much difference that one attempt would make. We can then use R to simulate doing this a large number of times, and then simply average the results! Thus: we generate a large number of samples, to represent a large number of investigations by DCP. We can then work out the difference in effectiveness between each &ldquo;new&rdquo; intervention and the current best option. The current best option is better than average, so we&rsquo;ll expect the difference in general to be negative. We can regard all those results as 0, however, as in that case there would (in our model) be no change in where the money goes. <img class="center" src="http://termsandtruthconditions.herokuapp.com//images/gwwc-dcp/gains-distribution.png" /> (Looking at the graph we get, the improvements also appear to be also log-normally distributed). Finally, we can then average these results. Call that average A.   </p>

<p>At this point, we need to plug in two final, very important parameters. These are figures for the money moved (M), and for the cost of DCP doing an investigation (C). These are particularly important as the our final estimate is going to be the average good done by an investigation (A*M), divided by the cost of doing it (C). Our final estimate will therefore be directly proportional to M, and inversely proportional to C. We&rsquo;ll discuss these a little more in a future post. For now, I&rsquo;m just going to give you some figures that are at least the right order of magnitude: namely about $50 million for M, and about $200,000 for C.</p>

<p>Putting all this together, we get an estimate for the effectiveness of DCP of 4.2 DALYs/$. For comparison, the best intervention that DCP claims to have found so far clocks in at 0.33 DALYs/$! As mentioned earlier, of course, this estimate is only as good as our model, but it certainly looks promising. In the next post I&rsquo;ll talk about how we can improve the model in a couple of ways.</p>

<p>A few final caveats:</p>

<ul>
  <li>This kind of research will be a high-variance strategy: most of the time nothing useful will happen, but occasionally there will be really, really good outcomes. </li>
  <li>The hidden parameter in this model is the effectiveness of DCP&rsquo;s <em>current</em> strongest recommendation. In particular, if new interventions are discovered that have much higher effectiveness, then the need to do further research drops.</li>
  <li>These results are not immediately applicable to charity-evaluators such as GiveWell. GiveWell looks at charities, not interventions, and we don&rsquo;t have a data-set for that in the way that we have for DCP. It may be that if we make some assumptions we might be able to adapt the data, but this is an open question.</li>
</ul>

<p><a href="http://termsandtruthconditions.herokuapp.com//downloads/code/dcp-effectiveness-1.R">Script.</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[sculptures by the sea]]></title>
    <link href="http://termsandtruthconditions.herokuapp.com//blog/2012/10/28/sculptures-by-the-sea/"/>
    <updated>2012-10-28T23:36:00+11:00</updated>
    <id>http://termsandtruthconditions.herokuapp.com//blog/2012/10/28/sculptures-by-the-sea</id>
    <content type="html"><![CDATA[<p>Around this time of year in Sydney they run a huge sculpture exhibition from Tamarara bay around the headland to Bondi beach. We had a work barbeque and expedition to see the sculptures, which were pretty awesome!</p>

<p><img src="http://termsandtruthconditions.herokuapp.com//images/sculptures-by-the-sea/IMG20121027115708.jpg" title="Seriously, this is 20 minutes from the CBD!" /></p>

<!-- more -->

<p>Here are a few that I liked (apologies for the poor quality, my camera couldn&rsquo;t really cope with how sunny it was!):</p>

<p><img src="http://termsandtruthconditions.herokuapp.com//images/sculptures-by-the-sea/IMG_20121027_135836.jpg" />
It&rsquo;s a giant 3d zipper&hellip; called &ldquo;Spinal cord&rdquo;? Okay, it&rsquo;s four people&rsquo;s spines being zipped together. That&rsquo;s a lot weirder.</p>

<p><img src="http://termsandtruthconditions.herokuapp.com//images/sculptures-by-the-sea/IMG_20121027_140705.jpg" />
This looks like the shiny twins of the monoliths from <em>2001</em> decided they wanted a beach party. The effect of looking through is pretty cool, though.</p>

<p><img src="http://termsandtruthconditions.herokuapp.com//images/sculptures-by-the-sea/IMG_20121027_141359.jpg" />
It&rsquo;s a naked couple on a donkey&hellip; with dog and rabbit heads.</p>

<p><img src="http://termsandtruthconditions.herokuapp.com//images/sculptures-by-the-sea/IMG_20121027_142547.jpg" />
This was surprisingly effective juxtaposed against the spectacular view.</p>

<p><img src="http://termsandtruthconditions.herokuapp.com//images/sculptures-by-the-sea/IMG_20121027_144127.jpg" />
A skeleton with a balance pole. Of course. Couldn&rsquo;t be having the poor guy falling off, could we?</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[First day at work]]></title>
    <link href="http://termsandtruthconditions.herokuapp.com//blog/2012/10/23/first-day-at-work/"/>
    <updated>2012-10-23T22:16:00+11:00</updated>
    <id>http://termsandtruthconditions.herokuapp.com//blog/2012/10/23/first-day-at-work</id>
    <content type="html"><![CDATA[<p>Today was my first day at Atlassian!</p>

<p><img src="http://termsandtruthconditions.herokuapp.com//images/welcome-pack.jpg" title="This was on my desk when I arrived... yes that is a Nerf gun!" /></p>

<!-- more -->
<p>Having managed to locate their offices, which they&rsquo;ve cunningly relocated (to <a href="https://maps.google.co.uk/maps?hl=en&amp;q=30+cambridge+street+enmore&amp;ie=UTF-8&amp;hq=&amp;hnear=0x6b12b038dc59e41f:0x8c7df8e24b3fe91d,30+Cambridge+St,+Enmore+NSW+2042,+Australia&amp;gl=uk&amp;ei=u9-DULq2GsKiigfrzYGQBw&amp;ved=0CCEQ8gEwAA">here</a>) as of a few weeks ago, I successfully made it into work. </p>

<p>The Atlassian offices stretch over several floors: the floor with reception on has loads of communal space with pool and ping-pong tables, and a huge kitchen full of free food. If I end up getting in early enough for breakfast, I think I may end up eating two meals a day from there!</p>

<p>The other floors are big, open-plan rooms (even the co-CEOs don&rsquo;t have offices!), with meeting rooms off to the sides for extended chats. Even the meeting rooms are cool: they&rsquo;re all named for geek references depending on their height. So the ones on the lowest floor are called &ldquo;Bag End&rdquo;, &ldquo;Rapture&rdquo; etc., whereas the top floor has the &ldquo;Normandy&rdquo;, &ldquo;Death Star&rdquo; etc. And all the doors have an appropriate stylised rendition of their name on them. They&rsquo;re quite good, I think Atlassian must have some pretty good artists and graphic designers around.</p>

<p>I&rsquo;ve also now finally got to find out what I&rsquo;m going to be spending all my time doing. The current plan is for me to spend three months with the Performance team and three months with the OnDemand team. The Performance guys are a kind of free-floating team; they don&rsquo;t make any products that get sold to customers, but they spend a lot of time trying to help everyone else make their products better. The OnDemand team are responsible for Atlassian&rsquo;s SaaS (Software as a Service) products: basically, if you want to use the Atlassian products via the web and have them hosted by Atlassian, rather than running them yourself, then the OnDemand team handles that.</p>

<p>My first project in the Performance team is going to be pretty open-ended. They&rsquo;ve got a lot of data about the usage and performance of their servers that hasn&rsquo;t really been properly mined yet, and they want me to have a go at trying to actually extract some useful knowledge from it. I have a feeling I may find myself once again cursing my lack of stats knowledge!</p>

<p>In other news: I&rsquo;ve found somewhere to live, finally! Camping out in youth hostels was getting a bit tedious, and I&rsquo;ve actually been staying with some friends of my parents for the last couple of nights (big thanks to Manuel and Gabi for putting me up!). But I&rsquo;m now ensconced in a pretty good location, so hopefully that&rsquo;s mostly sorted.</p>

<p>With the necessities looked after, I&rsquo;m also looking for more stuff to do around Sydney. I&rsquo;ve tracked down a choir: the <a href="http://www.sydneyphilharmonia.com.au/thechoirs.html">Sydney Philharmonia Choirs</a> look like they&rsquo;ve got a decent standard without being too serious. Most of the other decent ones I&rsquo;ve found look like they&rsquo;re more at the semi-pro level, and I&rsquo;m not sure I want <em>that</em> much involvement! In a fit of efficiency, I&rsquo;m auditioning tomorrow. </p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[The Value of Truth]]></title>
    <link href="http://termsandtruthconditions.herokuapp.com//blog/2012/10/19/the-value-of-truth/"/>
    <updated>2012-10-19T15:25:00+11:00</updated>
    <id>http://termsandtruthconditions.herokuapp.com//blog/2012/10/19/the-value-of-truth</id>
    <content type="html"><![CDATA[<blockquote>
  <p>The Will to Truth, which is to tempt us to many a hazardous enterprise, the famous Truthfulness of which all philosophers have hitherto spoken with respect, what questions has this Will to Truth not laid before us! What strange, perplexing, questionable questions! &hellip; In fact we made a long halt at the question as to the origin of this Will-until at last we came to an absolute standstill before a yet more fundamental question. We inquired about the VALUE of this Will. Granted that we want the truth: WHY NOT RATHER untruth? And uncertainty? Even ignorance? &ndash; <em>Beyond Good and Evil</em>, 1.1</p>
</blockquote>

<p>Of all the insights I gained from reading Nietzsche (almost all, I hasten to add, due to him making me think, not being <em>right</em>), the one that has stuck with me the most is almost the first thing you read when you open <em>Beyond Good and Evil</em>. Nietzsche raises a question that I don&rsquo;t think had even occurred to me the first time I read it: what is the value of truth?</p>

<!-- more -->
<p>Nietzsche is mostly interested in slagging off all past philsophers (going so far as to name names, extensively), but the starting point of his critique is the claim that all philosophical activity rests on a set of unjustified presumptions. To even get started doing philosophy, we have to believe that what we are doing is <em>worthwhile</em>, that it is <em>good</em>. And the most basic of these presuppositions is that it is good to believe the truth. </p>

<p>This is not normally a controversial claim. One of the fundamental tenets of modern liberal thought is that the truth is good for us (&ldquo;The truth will set you free&rdquo;). But <em>why</em> is that? As Nietzsche says: &ldquo;Why nor rather untruth? And uncertainty? Even ignorance?&rdquo;. To refuse even to ask these questions is to simply <em>fetishize</em> truth.</p>

<blockquote>
  <p>In spite of all the value which may belong to the true, the positive, and the unselfish, it might be possible that a higher and more fundamental value for life generally should be assigned to pretence, to the will to delusion, to selfishness, and cupidity. &ndash; <em>BGE</em>, 1.2</p>
</blockquote>

<p>We needn&rsquo;t go the whole way with Nietzsche here, but he paints the picture well enough. It certainly seems possible that it could be bad to believe the truth; or at least we haven&rsquo;t argued that it can&rsquo;t be, yet.</p>

<p>The second strand of Nietzsche&rsquo;s argument is to claim that the only way we can decide this issue is by assessing whether the pursuit of the truth furthers our values, and thus at the very foundation of philosophy we have slipped in a <em>moral</em> premiss.</p>

<blockquote>
  <p>It has gradually become clear to me what every great philosophy up till now has consisted of - namely, the confession of its originator, and a species of involuntary and unconscious auto-biography; and moreover that the moral (or immoral) purpose in every philosophy has constituted the true vital germ out of which the entire plant has always grown. Indeed, to understand how the abstrusest metaphysical assertions of a philosopher have been arrived at, it is always well (and wise) to first ask oneself: &ldquo;What morality do they (or does he) aim at?&rdquo; &ndash; <em>BGE</em>, 1.6</p>
</blockquote>

<p>Nietzsche, then, leaves us with two questions:</p>

<ol>
  <li>Is the truth valuable?</li>
  <li>How are we to decide that question?</li>
</ol>

<p>TODO: talk about 2 first, it&rsquo;s more fundamental!</p>

<p>1.</p>

<p>In general, if we&rsquo;re thinking about how we should behave, there are two kinds of considerations we could appeal to: prudential and moral. As I&rsquo;ve argued, there&rsquo;s no way around making assumptions about these, so let&rsquo;s make some! I&rsquo;m going try and get away with using examples that are as general as possible, but I&rsquo;m going to assume that maximizing expected satisfaction of our preferences is what we ought, prudentially, to do, and that maximizing some kind of aggregation of people&rsquo;s personal welfare is what we ought, morally, to do.<sup id="fnref:consequentialism"><a href="#fn:consequentialism" rel="footnote">1</a></sup> I&rsquo;m going to use moral and prudential examples pretty interchangeably; for a consequentialist the two are fairly continous, it&rsquo;s just a case of changing whether one is aiming only at one&rsquo;s own good, or at everyone&rsquo;s good. </p>

<p>On the face of it, it seems pretty weird that believing the truth might be bad for you. Knowing more means that your map of the world matches the world better, and so you can act with more confidence in how your actions will affect the world, and hence achieve your aims better. The <em>benefits</em> of believing the truth in general seem pretty clear. There&rsquo;s even a theorem about it proved by I.J. Good: for a Bayesian agent, gathering more information before acting is always the option that maximizes utility, so long as the cost of doing so is sufficiently small.<sup id="fnref:Good"><a href="#fn:Good" rel="footnote">2</a></sup> In particular, if you could get more information for free, you should always do so!</p>

<p>On the other hand, it doesn&rsquo;t seem that hard to come up with a counterexample:</p>

<blockquote>
  <p><strong>Omega 1</strong>: </p>

  <p>An nigh-omnipotent extra-galactic superintelligence named Omega threatens to torture everyone on Earth for eternity unless, by next Tuesday, you come to believe that you are a salmon.</p>
</blockquote>

<p>In this case, it seems to be pretty clear that it would be better to believe that you are a salmon: while that might make life pretty difficult for you, it&rsquo;s clearly better than eternal torture for everyone. So what&rsquo;s going on?</p>

<p>There are a couple of ways in which Good&rsquo;s theorem can fail to apply, and these nicely split things up:</p>

<ol>
  <li>The agent may not be a perfect Bayesian agent.</li>
  <li>The act of gathering information, or the simple possession of it, may change the situation.</li>
</ol>

<p>Both of these can apply. The first is less interesting: obviously humans are nowehre near perfect Bayesian agents, and so there is no guarantee that gaining more information will actually lead to us making better decisions. There are a whole host of biases that interfere with good decision-making; the <a href="http://wiki.lesswrong.com/wiki/Bias">LessWrong Wiki</a> has a good collection of material on the topic. There are plenty of wholly mundane examples:</p>

<blockquote>
  <p><strong>Depression</strong>: </p>

  <p>Your boss informs you that you&rsquo;re about to be fired in a wave of redundancies. But if you work extremely hard he might be able to justify keeping you. However, he can only do this for about 1% of the employees. Instead of working hard, this knowledge sends you into a fit of depression, and you do nothing.</p>
</blockquote>

<p>(Strictly, it&rsquo;s a bit unclear which category this falls under. Is it a case of you failing at acting so as to best pursue your goals, or is it an example of your epistemic state affecting your options by rendering some of them emotionally impossible? In general, there isn&rsquo;t always going to be a clear-cut distinction here.)</p>

<p>The other possibility is more interesting, and it&rsquo;s what&rsquo;s going on in Omega 1. Sometimes an agent&rsquo;s <em>own epistemic state</em> can affect their situation. In the Omega 1 case, Omega explicitly bases its actions upon your beliefs. But these kind of problems, where what is in your head affects what is outside, have bedevilled decision theorists and moral philsophers for a long time. For which reason we&rsquo;re going to take a whirlwind detour, after whch hopefully we&rsquo;ll be able to draw out some similarities.</p>

<h2 id="decision-theory">Decision theory</h2>

<p>In decision theory, matters get very complicated once it is possible for the situation to depend upon the mental states of the agents. Often the problems faced are actually more general than what we&rsquo;ve been considering so far: rather than just the agent&rsquo;s epistemic state, the situation may be conditional on some other aspect of their nature. Consider the following famous example:</p>

<blockquote>
  <p><strong>Omega 2 (Newcomb&rsquo;s Problem)</strong>: </p>

  <p>Omega offers you the chance to take one or both of two boxes, A and B. B contains $1000 no matter what. A contains $1 million iff Omega predicts that you will take only box A. (Omega is, as far as you know, an infallible predictor).</p>
</blockquote>

<p>In this case, Omega&rsquo;s actions are conditional on the agent&rsquo;s <em>decision procedure</em>: if I am the sort of agent who will take only box A in this circumstance, then Omega will fill it with $1 million, otherwise not. Newcomb&rsquo;s problem is famous for tying decision theories in knots. Under many analyses it seems obvious that one should two-box: at the time you make your decision, either Omega has filled box A or not; either way, you come out $1000 ahead if you take both boxes. However, annoyingly, agents who reason like this will end up with only $1000, whereas those who one-box will (entirely predictably) end up with the million.</p>

<p>In general, situations like this where it&rsquo;s advantageous to behave otherwise than simply maximizing for the situation one is currently in can be handled by making <em>precommitments</em>. Consider a simpler example:</p>

<blockquote>
  <p><strong>Chicken</strong>: </p>

  <p>You and an advesary are competing in a game of chicken for a great prize. The rules are as follows: you both drive towards each other as fast as possible, and the first person who swerves aside loses.</p>
</blockquote>

<p>If you are given a chance to prepare for such a contest, what should you do? You should drink yourself blind, tear out the steering wheel and blindfold yourself. If your opponent is truly convinced that you <em>will not</em> turn aside, then their options are much simpler: turn aside themselves, or cause both of you to die. If they&rsquo;re behaving rationally, then they&rsquo;ll turn aside, and you will claim your prize (unless they managed to tear out their steering wheel first!). Similarly, if you know that Omega is coming, then you should swear blind that you will one-box, perhaps going so far as to get a reliable third party to promise to shoot you if you do not do so. That is, you should <em>make yourself</em> into the kind of agent that will one-box, and hence end up with the money.</p>

<p>Precommitments can be counter-intuitive because they tend to involve committing yourself in the future to one course of action, even though at that point it would be better for you to take another one. For another example, consider:</p>

<blockquote>
  <p><strong>Hitch-hiker</strong>: </p>

  <p>You are stuck in the desert with no source of water or means of communication, without which you will surely die. A passer-by offers to drive you into town for $100. You don&rsquo;t have the money on you, and you are a perfectly selfish person who acts only as will best further their interests. So you know that once you get back to town you will refuse to pay your rescuer, as there would then be no downside for you. Unfortunately, you&rsquo;re also a terrible liar, and no matter how you promise, your potential rescuer believes (correctly) that you won&rsquo;t pay him. And so you are left in the desert to die.<sup id="fnref:Parfit"><a href="#fn:Parfit" rel="footnote">3</a></sup></p>
</blockquote>

<p>In this case, being able to precommit to paying the passer-by, even though once you get back to town such an action would be sub-optimal for you, is an extremely good idea, if you can manage it! Indeed, arguably one of the useful features of social norms in favour of keeping your promises is that it helps us negotiate this kind of situation.<sup id="fnref:TDT"><a href="#fn:TDT" rel="footnote">4</a></sup> </p>

<p>It&rsquo;s no coincidence that the slogan I&rsquo;ve given you for making general precommitments is one of the informal ways of understanding what&rsquo;s going on with TDT, a decision procedure invented by Eliezer Yudkowsky for dealing with these kinds of problems. </p>

<h2 id="moral-philosophy">Moral philosophy</h2>

<p>In modern discussions about consequentialism, much ink has been spilt worrying about what consequentialist agents would be like. In particular, it seems like a &ldquo;perfect&rdquo; consequentialist agent (one who always tried to act so as to maximize the good) would be a bit of a dick. Constantly calculating whether the consequences of one&rsquo;s actions are optimal sometimes seems like fundamentally the wrong attitude to take, particularly in many human relationships. It seems hard to see how such a person could, for example, really be in <em>love</em>. (&ldquo;I nearly got you a birthday present, darling, but then I realised the money would do more good if I sent it to starving children in Africa&rdquo;). Even if there were no morally overriding constraints, such a person might just be constitutionally incapable of loving. (&ldquo;I decided nobody else needed the money more, so I actually bought you a present!&rdquo;) This seems bad! <sup id="fnref:BW"><a href="#fn:BW" rel="footnote">5</a></sup></p>

<p>Again, there are a couple of ways in which being such an agent could lead to bad consequences. Firstly, calculating consequences is hard, and can be slow or just downright intractable. There are times when the best thing to do is act swiftly, and an agent with a simple dispostion to do so would be better overall than one who calculated. For example, taking the time to calculate whether it is worth saving a drowning person may simply result in them dying. In this case adopting a simple heuristic - save people where there&rsquo;s trivial inconvenience - may lead to much better outcomes in general.</p>

<p>On the other hand, </p>

<p>And behind all logic and its seeming sovereignty of movement, there are valuations, or to speak more plainly, physiological demands, for the maintenance of a definite mode of life For example, that the certain is worth more than the uncertain, that illusion is less valuable than &ldquo;truth&rdquo;&hellip;</p>

<p>The falseness of an opinion is not for us any objection to it: it is here, perhaps, that our new language sounds most strangely. The question is, how far an opinion is life-furthering, life-preserving, species-preserving, perhaps species-rearing&hellip;</p>

<div class="footnotes">
  <ol>
    <li id="fn:consequentialism">
      <p>Yep, I&rsquo;m assuming some kind of consequentialism here. If you&rsquo;re a non-consequentialist, a lot of what I&rsquo;m going to say won&rsquo;t apply to you. At some point I&rsquo;ll try and present some more argument for why you should be a consequentialist, but in the mean time you can try <a href="http://www.raikoth.net/consequentialism.html">Yvain&rsquo;s Consequentialism FAQ</a>.<a href="#fnref:consequentialism" rel="reference">&#8617;</a></p>
    </li>
    <li id="fn:Good">
      <p>Good, I.J. (1967) QOn the Principle of Total Evidence.<a href="#fnref:Good" rel="reference">&#8617;</a></p>
    </li>
    <li id="fn:Parfit">
      <p>This example comes from Parfit&rsquo;s <em>Reasons and Persons</em> - Parfit was one of the first moral philosophers to discuss the kind of &ldquo;rational irrationality&rdquo; that we&rsquo;re talking about here.<a href="#fnref:Parfit" rel="reference">&#8617;</a></p>
    </li>
    <li id="fn:TDT">
      <p>The problem with precommitments is that they need to be made beforehand. They often don&rsquo;t work if you&rsquo;re already facing the situation (or worse, if the other guy commits before you do!). We might then wonder if there&rsquo;s any way to reap the amazing benefits of precommitments without having to know what&rsquo;s coming. We could try and work out lots of general precommitments that we could make, which is hard. Or we could just make a <em>fully general</em> precommitment: say something like &ldquo;I will always act in the way that I would like to have precommitted to act&rdquo;. If you can truly precommit in this way, then you can come out on top in a whole bunch of tricky situations.<a href="#fnref:TDT" rel="reference">&#8617;</a></p>
    </li>
    <li id="fn:BW">
      <p>This kind of argument is usually sourced from Bernard Williams (AKA the most overrated philosopher ever), and is often phrased in terms of &ldquo;integrity&rdquo;. The classic consequentialist response is Raliton&rsquo;s <em>Alienation, Consequentialism and the Demands of Morality</em>.<a href="#fnref:BW" rel="reference">&#8617;</a></p>
    </li>
  </ol>
</div>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[The Value of Truth]]></title>
    <link href="http://termsandtruthconditions.herokuapp.com//blog/2012/10/15/the-value-of-truth/"/>
    <updated>2012-10-15T15:16:00+11:00</updated>
    <id>http://termsandtruthconditions.herokuapp.com//blog/2012/10/15/the-value-of-truth</id>
    <content type="html"><![CDATA[
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Introduction]]></title>
    <link href="http://termsandtruthconditions.herokuapp.com//blog/2012/10/09/introduction/"/>
    <updated>2012-10-09T16:35:00+11:00</updated>
    <id>http://termsandtruthconditions.herokuapp.com//blog/2012/10/09/introduction</id>
    <content type="html"><![CDATA[<p>Hi all!</p>

<p>I&rsquo;m Michael Peyton Jones, and this is my blog. I&rsquo;m planning to use it for a variety of stuff, but I&rsquo;m also going to be using it as a way to keep people up to date with what I&rsquo;m doing in Australia. If that&rsquo;s you, then you can get a feed that will just get you those posts by clicking <a href="http://termsandtruthconditions.herokuapp.com//blog/categories/australia/atom.xml">this link</a>. If you&rsquo;d like to subscribe by email instead, then use <a href="http://feedburner.google.com/fb/a/mailverify?uri=AustraliaTermsAndTruthConditions&amp;loc=en_US">this link</a> to do that. If you&rsquo;re willing to take the risk of being subscribed to the whole blog (warning: probably irregular and inconsistent content!), then there&rsquo;s a RSS link in the navigation bar.</p>
]]></content>
  </entry>
  
</feed>
